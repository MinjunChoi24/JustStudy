import os
import time
import kagglehub
import pandas as pd
import numpy as np
import joblib
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.svm import LinearSVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import MultinomialNB
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, accuracy_score
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
from xgboost import XGBClassifier

# ==========================================
# 0. ì„¤ì • ë° ìƒìˆ˜ ì •ì˜ (Configuration)
# ==========================================
MODEL_SAVE_DIR = "../saved_models"
if not os.path.exists(MODEL_SAVE_DIR):
    os.makedirs(MODEL_SAVE_DIR)


def load_and_preprocess_data():
    """ë°ì´í„° ë‹¤ìš´ë¡œë“œ, ë³‘í•©, ì „ì²˜ë¦¬ ë° ë°¸ëŸ°ì‹±ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤."""
    print("=" * 50)
    print("ğŸ“¥ 1. ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ ë° ë¡œë”© ì¤‘...")

    # Kaggle ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ
    path_news = kagglehub.dataset_download("ankurzing/sentiment-analysis-for-financial-news")
    df_news = pd.read_csv(f"{path_news}/all-data.csv", encoding='latin1', header=None, names=['Sentiment', 'Text'])

    path_tweets = kagglehub.dataset_download("yash612/stockmarket-sentiment-dataset")
    df_tweets = pd.read_csv(f"{path_tweets}/stock_data.csv")

    # ì „ì²˜ë¦¬
    tweet_map = {-1: 'negative', 1: 'positive'}
    df_tweets['Sentiment'] = df_tweets['Sentiment'].map(tweet_map)
    df_tweets = df_tweets[['Text', 'Sentiment']]

    # ë³‘í•©
    print("\nğŸ”— ë°ì´í„° ë³‘í•© ì¤‘...")
    df_total = pd.concat([df_tweets, df_news], ignore_index=True)
    df_total.dropna(subset=['Text', 'Sentiment'], inplace=True)

    # 1. ì¶©ëŒ ë¼ë²¨ ì œê±°
    conflict_mask = df_total.groupby("Text")["Sentiment"].nunique() > 1
    conflict_texts = conflict_mask[conflict_mask].index
    df_total = df_total[~df_total["Text"].isin(conflict_texts)]

    # 2. ë‚˜ë¨¸ì§€ ì¤‘ë³µ ì œê±°
    df_total = df_total.drop_duplicates(subset='Text')

    # ë¼ë²¨ ì¸ì½”ë”©
    print("ğŸ”¢ ë¼ë²¨ ì¸ì½”ë”© ë³€í™˜...")
    label_mapping = {'negative': 0, 'neutral': 1, 'positive': 2}
    df_total['Sentiment_Encoded'] = df_total['Sentiment'].map(label_mapping)

    # Train / Test ë¶„ë¦¬ (Stratify)
    X = df_total['Text']
    y = df_total['Sentiment_Encoded']

    X_train_raw, X_test_raw, y_train_raw, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42, stratify=y
    )

    print(f"âœ‚ï¸ ë°ì´í„° ë¶„ë¦¬ ì™„ë£Œ (Train: {len(X_train_raw)}, Test: {len(X_test_raw)})")

    return X_train_raw, y_train_raw, X_test_raw, y_test


def vectorize_text(X_train, X_test):
    """TF-IDF ë²¡í„°í™”ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤."""
    print("\n" + "=" * 50)
    print("âš¡ 2. TF-IDF ë²¡í„°í™” ìˆ˜í–‰ ì¤‘...")

    vectorizer = TfidfVectorizer(
        max_features=10000,
        ngram_range=(1, 2),
        sublinear_tf=True,
        min_df=3,
        stop_words=None
    )

    X_train_vec = vectorizer.fit_transform(X_train)
    X_test_vec = vectorizer.transform(X_test)

    print(f"   -> ìƒì„±ëœ Feature ê°œìˆ˜: {X_train_vec.shape[1]}")
    return X_train_vec, X_test_vec, vectorizer


def train_and_evaluate(model, name, X_train, y_train, X_test, y_test):
    """ëª¨ë¸ì„ í•™ìŠµí•˜ê³  í‰ê°€ ê²°ê³¼ë¥¼ ì¶œë ¥í•˜ëŠ” ê³µí†µ í•¨ìˆ˜ì…ë‹ˆë‹¤."""
    print("\n" + "-" * 50)
    print(f"ğŸš€ [{name}] í•™ìŠµ ë° í‰ê°€ ì‹œì‘...")

    if name == "XGBoost":
        print("   -> Early Stopping ì ìš© ì¤‘...")
        model.fit(
            X_train, y_train,
            eval_set=[(X_test, y_test)],
            verbose=False
        )
        print(f"   -> ìµœì ì˜ íŠ¸ë¦¬ ê°œìˆ˜(Best Iteration): {model.best_iteration}")
    else:
        model.fit(X_train, y_train)

    y_pred = model.predict(X_test)
    acc = accuracy_score(y_test, y_pred)

    print(f"   -> ì •í™•ë„(Accuracy): {acc:.4f}")
    print("\n[Classification Report]")
    print(classification_report(y_test, y_pred, target_names=['Negative', 'Neutral', 'Positive']))

    print(f"ğŸ“Š [{name}] í˜¼ë™í–‰ë ¬ ì¶œë ¥ ì¤‘...")
    cm = confusion_matrix(y_test, y_pred)
    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['Negative', 'Neutral', 'Positive'])
    disp.plot(cmap=plt.cm.Blues)
    plt.title(f"{name} Confusion Matrix")
    plt.show()

    return model, acc


def run_svm_grid_search(X_train, y_train, X_test, y_test):
    """SVM GridSearch ìˆ˜í–‰ (ê°€ì¥ ì„±ëŠ¥ì´ ì¢‹ì•˜ë˜ ëª¨ë¸)"""
    print("\n" + "=" * 50)
    print("ğŸ” 4. SVM GridSearch (í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹) ìˆ˜í–‰ ì¤‘...")

    param_grid = {
        'C': [0.1, 0.5, 1, 5, 10, 50],
        'class_weight': [None, 'balanced']
    }

    base_svm = LinearSVC(random_state=42, dual=False, max_iter=3000)
    grid = GridSearchCV(base_svm, param_grid, cv=5, scoring='f1_macro', n_jobs=-1, verbose=1)

    grid.fit(X_train, y_train)

    best_model = grid.best_estimator_
    y_pred = best_model.predict(X_test)

    print("\nğŸ† [SVM ìµœì¢… íŠœë‹ ê²°ê³¼]")
    print(f"   -> Best Parameters: {grid.best_params_}")
    print(f"   -> ìµœì¢… ì •í™•ë„: {accuracy_score(y_test, y_pred):.4f}")
    print(classification_report(y_test, y_pred, target_names=['Negative', 'Neutral', 'Positive']))

    print("ğŸ“Š [Best SVM] í˜¼ë™í–‰ë ¬ ì¶œë ¥ ì¤‘...")
    cm = confusion_matrix(y_test, y_pred)
    disp = ConfusionMatrixDisplay(confusion_matrix=cm,
                                  display_labels=['Negative', 'Neutral', 'Positive'])
    disp.plot(cmap=plt.cm.Blues)
    plt.title("Best SVM Confusion Matrix")
    plt.show()

    return grid, best_model


# ==========================================
# ë©”ì¸ ì‹¤í–‰ ë¸”ë¡
# ==========================================
if __name__ == "__main__":
    # 1. ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬
    X_train_text, y_train, X_test_text, y_test = load_and_preprocess_data()

    # 2. ë²¡í„°í™”
    X_train_vec, X_test_vec, vectorizer = vectorize_text(X_train_text, X_test_text)

    # ---------------------------------------------------------
    # [ì¶”ê°€ëœ ë¶€ë¶„] 3. íŠœë‹ ì „ Baseline SVM í‰ê°€
    # ---------------------------------------------------------
    print("\n" + "=" * 50)
    print("ğŸ 3. Baseline SVM (íŠœë‹ ì „) ì„±ëŠ¥ í‰ê°€")

    # ê¸°ë³¸ê°’ SVM (dual=FalseëŠ” ìƒ˜í”Œ ìˆ˜ê°€ ë§ì„ ë•Œ ê¶Œì¥ë¨, max_iterëŠ” ìˆ˜ë ´ ê²½ê³  ë°©ì§€ìš©)
    baseline_svm = LinearSVC(random_state=42, dual=False, max_iter=3000)

    train_and_evaluate(
        baseline_svm,
        "Baseline SVM (No Tuning)",
        X_train_vec, y_train, X_test_vec, y_test
    )
    # ---------------------------------------------------------

    # 4. SVM ë©”ì¸ ëª¨ë¸ í•™ìŠµ (GridSearch)
    svm_grid, best_svm = run_svm_grid_search(X_train_vec, y_train, X_test_vec, y_test)

    # 5. ëª¨ë¸ ì €ì¥ (ë¡œì»¬ ê²½ë¡œ)
    print("\nğŸ’¾ ëª¨ë¸ ì €ì¥ ì¤‘...")
    joblib.dump(svm_grid, os.path.join(MODEL_SAVE_DIR, 'my_svm_model.pkl'))
    joblib.dump(vectorizer, os.path.join(MODEL_SAVE_DIR, 'my_tfidf_vectorizer.pkl'))
    print(f"   -> ì €ì¥ ìœ„ì¹˜: {os.path.abspath(MODEL_SAVE_DIR)}")

    # 6. ë¹„êµìš© ë‹¤ë¥¸ ëª¨ë¸ë“¤ ì‹¤í–‰ (XGBoost ë“±)
    print("\n" + "=" * 50)
    print("ğŸ 5. ë‹¤ë¥¸ ì•Œê³ ë¦¬ì¦˜ê³¼ ë¹„êµ í‰ê°€")

    models = {
        "XGBoost": XGBClassifier(
            n_estimators=1000,
            learning_rate=0.05,
            max_depth=6,
            random_state=42,
            n_jobs=-1,
            eval_metric='mlogloss',
            early_stopping_rounds=50)
    }

    results = {}
    for name, model in models.items():
        _, acc = train_and_evaluate(model, name, X_train_vec, y_train, X_test_vec, y_test)
        results[name] = acc

    print("\nğŸ“Š [ìµœì¢… ëª¨ë¸ë³„ ì •í™•ë„ ìˆœìœ„]")
    sorted_results = sorted(results.items(), key=lambda x: x[1], reverse=True)
    for name, score in sorted_results:
        print(f"{name}: {score:.4f}")